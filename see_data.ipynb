{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Flowers102 Dataset With Caption\n",
    "\n",
    "The Text-to-Image Synthesis \n",
    "- [Original repo](https://github1s.com/reedscot/icml2016)\n",
    "- [Example repo](https://github1s.com/aelnouby/Text-to-Image-Synthesis/tree/master/images)\n",
    "    - Download Flowers102 [here](https://drive.google.com/file/d/1EgnaTrlHGaqK5CCgHKLclZMT_AMSTyh8/view)\n",
    "\n",
    "The [paper](https://arxiv.org/pdf/1605.05395.pdf) that first published the annotated Flower102 dataset\n",
    "\n",
    "Please perform the following steps to correctly download the dataset.\n",
    "\n",
    "1. Create a `./data` directory under the root directory of this repo. Run `mkdir data && cd ./data`\n",
    "2. Goto [this repo](https://github1s.com/aelnouby/Text-to-Image-Synthesis/tree/master/images), examine its README.md, then proceed to the [download link](https://drive.google.com/file/d/1EgnaTrlHGaqK5CCgHKLclZMT_AMSTyh8/view)\n",
    "3. Verify the file is successfully downloaded, and resides in `./data/flowers.hdf5`\n",
    "4. Install [h5py](https://pypi.org/project/h5py/) using `pip install h5py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Examine Items in Flowers.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The [train] set has [29390] images, e.g. ['image_00001_0', 'image_00001_1', 'image_00001_2'] ...\n",
      "The [valid] set has [5780] images, e.g. ['image_03369_0', 'image_03369_1', 'image_03369_2'] ...\n",
      "The [test] set has [5775] images, e.g. ['image_03095_0', 'image_03095_1', 'image_03095_2'] ...\n"
     ]
    }
   ],
   "source": [
    "import h5py, numpy as np\n",
    "\n",
    "''' 1. Get familiar with the hdf5 structure '''\n",
    "\n",
    "file = h5py.File('./data/flowers.hdf5', 'r')\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    ds_keys = [str(key) for key in file[split].keys()]\n",
    "    print(f'The [{split}] set has [{len(ds_keys)}] images, e.g. {ds_keys[:3]} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 keys in each item: [dict_keys(['right_images', 'right_embed', 'wrong_images', 'inter_embed', 'txt'])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'prominent purple stigma,petals are white inc olor\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import *\n",
    "\n",
    "''' 2. Get familiar with images, labels, captions \n",
    "    ref: https://github1s.com/aelnouby/Text-to-Image-Synthesis/blob/master/txt2image_dataset.py#L1-L11\n",
    "'''\n",
    "\n",
    "dataset = Flowers102('./data/flowers.hdf5')\n",
    "\n",
    "item = dataset.__getitem__(0)\n",
    "print(f'There are 5 keys in each item: [{item.keys()}]')\n",
    "item['txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import Flowers102\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataroot = \"./data/cvpr2016_flowers\"\n",
    "image_size = 64\n",
    "batch_size = 64\n",
    "noise_channels = 100\n",
    "num_workers = 4\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ])\n",
    "\n",
    "\n",
    "# dataset = Flowers102(root=dataroot, download='True')\n",
    "dataset = dset.ImageFolder(root=dataroot, transform = transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for im, lab in dataloader:\n",
    "    print(lab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c17ef166827587e58f5e1d9fc37d7d71128d9ee001cf728f6b987f1c2dd16782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
